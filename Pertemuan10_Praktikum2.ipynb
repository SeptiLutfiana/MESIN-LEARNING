{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXvTju3byg/1o5oaV+ljnu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeptiLutfiana/MESIN-LEARNING/blob/main/Pertemuan10_Praktikum2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator Teks dengan RNN"
      ],
      "metadata": {
        "id": "JuYK9vEwfWqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow"
      ],
      "metadata": {
        "id": "ieiKQngffbag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iw4Z6bxkfI-i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download data set\n",
        "path_to_file=tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68mG6VHefd58",
        "outputId": "de41ab65-bfc2-4683-8a6f-4e76bdc41f2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwR98L5Dfn0T",
        "outputId": "db5d49e9-07dc-4d05-b8a5-a02338fa43fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYs7GvcmfrR8",
        "outputId": "389d4f25-9601-4b64-af52-098c1ebae99f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39hTL51rfs8U",
        "outputId": "b0126d3a-d93a-4542-e20e-f9891fe437a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olah Teks"
      ],
      "metadata": {
        "id": "_-V8h_J0fxo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts=['abcdefg','xyz']\n",
        "chars=tf.strings.unicode_split(example_texts,input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8XgSgc4fyQC",
        "outputId": "fb2ef4ac-1e5b-45bf-ff8f-07ed7b051b82"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "reWQ1hYof78I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars=tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab),mask_token=None)"
      ],
      "metadata": {
        "id": "Zpln_Wbhf8fV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids=ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5OYJpOXgEDv",
        "outputId": "95ccd39a-3f5f-48d2-946a-5b78c1a24d85"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "aGloaMsRgJtY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars=chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoV5VFK5gJvh",
        "outputId": "c1ea9644-6fce-4bf0-cdfc-b75e29707d78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars,axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjOGVDdgJy_",
        "outputId": "33fbcae5-ee34-4da9-8cf1-72687a9b2ae2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids),axis=-1)"
      ],
      "metadata": {
        "id": "yB2DakR-gUE0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediksi"
      ],
      "metadata": {
        "id": "fkrxYSAggbkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids=ids_from_chars(tf.strings.unicode_split(text,'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8TGO4LRgcLu",
        "outputId": "e623bd38-a9f3-4dd1-d614-226a9514fb40"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset=tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "R3YlsWCaggC5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlVe2FBLgi9x",
        "outputId": "0b2dccb2-1a83-4eb0-a288-aa414c19e60c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100"
      ],
      "metadata": {
        "id": "BWN8YrG7gpkr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oin3gLENgsAx",
        "outputId": "e88e4425-3c41-41a6-e03f-5395b8f6a839"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kIOwTmlguMk",
        "outputId": "8f77b45d-7f09-48bf-f9ec-314d74015b51"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text,target_text"
      ],
      "metadata": {
        "id": "4yg1-G_ihN5b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBVhsDz0hmYL",
        "outputId": "d4d7c7da-58a4-4a25-98ed-ad367ac8c48a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "2Uz-QhEQhn-m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\",text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84x4dIryhtbY",
        "outputId": "b69efd9b-0411-44ca-de95-ace4ac036641"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat Batch Training"
      ],
      "metadata": {
        "id": "j0TnPznMiAth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAe5LmR5iBNE",
        "outputId": "2bb2e73a-7f44-48da-c30e-4e70a04975e6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat Model"
      ],
      "metadata": {
        "id": "cwGufQOsklaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "ol7tAzPnklyB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "_fBm8yQ_krFs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "6NjXb1CzkubM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uji Model"
      ],
      "metadata": {
        "id": "li2IgrN4kyrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX5m2XAkkwbB",
        "outputId": "f096da7f-34b1-4a16-e5a7-5040d5a76c70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2CPamW6k3Cg",
        "outputId": "2d5baf6c-fb17-44c3-af8a-59125eba6fa4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices=tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices=tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "TqAf-ulHk4ua"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyyRojHIlB2o",
        "outputId": "c993b63f-ea42-4f1c-f513-52fffe26483f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37,  1, 34, 29, 56, 13, 27, 24,  7, 21, 14, 26, 28, 42, 44, 17, 62,\n",
              "       10, 49, 35, 60, 42, 10,  2, 64, 58, 19, 11, 57, 41, 29, 52, 25, 14,\n",
              "       39, 14, 12,  1, 33, 52,  2, 30, 22, 47, 16, 18, 33, 31, 27, 52,  4,\n",
              "       21, 15, 17, 11, 33, 38,  7,  8,  8, 25, 61, 21, 55, 51, 13, 39, 32,\n",
              "       41, 62, 25, 11, 26,  6, 56, 62, 36, 59,  6,  7, 37,  4, 29, 47, 23,\n",
              "       31, 26, 17, 12, 28, 28, 39,  1,  7, 42, 50, 56, 39, 45, 42])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\",text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\",text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jL3nRwDlD75",
        "outputId": "829effa1-c723-4e25-c3e0-4374beb9853a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"s to do here, Thomas tapster? let's withdraw.\\n\\nPOMPEY:\\nHere comes Signior Claudio, led by the provos\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"X\\nUPq?NK,HAMOceDw3jVuc3 ysF:rbPmLAZA;\\nTm QIhCETRNm$HBD:TY,--LvHpl?ZSbwL:M'qwWt',X$PhJRMD;OOZ\\n,ckqZfc\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "tH3KdXjqlIOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "0luN4ngrlI8u"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvgNACrklMHu",
        "outputId": "db65a460-1e1a-431f-bfff-3bdfba802bad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.191628, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gh_0rOulO_L",
        "outputId": "3bcfae22-0ca0-48e7-e119-b6aa588742f5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.13036"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "7aPYNKnylSBr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "8Slk5ij5lV4x"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20"
      ],
      "metadata": {
        "id": "W88fPivIlV7z"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=\n",
        " [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNIwT6uylbG2",
        "outputId": "6ea68b36-598e-4e7c-c720-58879be2c4fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 51ms/step - loss: 2.7395\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 2.0043\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 1.7291\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.5650\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.4645\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.3940\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.3418\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2967\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.2563\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.2176\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.1791\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1391\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0976\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0535\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0076\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 0.9584\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9068\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8548\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8018\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Teks"
      ],
      "metadata": {
        "id": "xRjCWTrElip9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "dRrr_YnaljL2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model=OneStep(model,chars_from_ids,ids_from_chars)"
      ],
      "metadata": {
        "id": "rxeyk1nhpQoE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOetzVdxpWCA",
        "outputId": "137d7040-2314-48d7-be09-27b97a4925d9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Tybalt, believe this, that my foul way to be one.\n",
            "\n",
            "POMPEY:\n",
            "Pray, sir, what hath bestock me? I speak too Kate:\n",
            "I have some weapons that a careful store\n",
            "Of that, and matter well owes;\n",
            "Which, being that, by great ones all itself.\n",
            "Ah, traitors farewell and heart, thy sorrow,\n",
            "That you be accused and too much\n",
            "wrinking paint.\n",
            "Come, come, dame, peace: thou art too furlones,\n",
            "Nor with a birth and notice of thine;\n",
            "For in this bost from me; thou art no brother's use:\n",
            "I'll have this, vowsor to what ends at hand:\n",
            "And all to him in heaven be out, and hearers as\n",
            "she is my son, and my process shore, bearing\n",
            "A place of water in the law doth learn,\n",
            "And let my foot can sink them o'.\n",
            "I thought, as being assurance sped time\n",
            "With thine own toward till I have: I have\n",
            "here her, sir, that she be both of twelve,\n",
            "I do these worth of all his deeds,\n",
            "Not by my spider, that have I have the daith\n",
            "hereries are obsed and fair and inquired for some it:\n",
            "Bring me well, I had too long have:\n",
            "My trestle-moning. I cunning him \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.605544328689575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqGC4lI3pZsV",
        "outputId": "1867925d-9541-468d-d1e4-01e1940518a5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhy, then he hath done many to be much upon my house,\\nYou have neither distribute my heart!\\nA countenance of those grounds to't aid them.\\nWhy, how now, known love to the nouth!\\nSo whom you seek together, dancing,\\nspeaking, no greenion banished.\\nAre madness, my looring? is he master\\nThan Bolingbroke take up more strong thunder?\\nMust yet thou lovest not in the sun\\nAnd thou dost give thee so, my little bid\\nHave I did soil the choler in 'em; and, Master\\nEarth his good will to do some mine art thou,\\nLady for beasts, devover back and stood,\\nScarcist well becomes the heavy help,\\nAnd he hath sorrow then both publicle,\\nThou rough Bohemio in beauty's mine,\\nSure thou the vengeance plucks it bitterly;\\nOr, if thou becomes strong Angelo.\\n\\nDUKE OF YORK:\\nHo! who is not in these lamentation;\\nIf you'll tell the people who comes wear it.\\n\\nGRUMIO:\\nSir, your ancury word of these,\\nThat brought it after their demands,\\nRetirest that they there should be contented:\\nBut, sir, the duke shall be behonds to\\nOr an\"\n",
            " b\"ROMEO:\\nGo to, go along.\\n\\nAUTOLYCA:\\nMy back-word in his a dishonour 'Graugh,\\nHe is my power to breed out an\\nany manements where she yet not set\\nOur enemies on thee of tarklion.\\n\\nGREGORY:\\nI raid our lace the fresh inhered that receive a fier\\nThat issue our footment made a feast: or if your knife hath sure\\nof a holy humour.\\n\\nLADY ANNE:\\nWould the world breat, I thank how to pass, but I am modest\\nas sounded dog. But who comfort how he left prologue.\\nAnd who in beasing at the Edpilour light\\nBe satisfied? why, not for love,\\nAnd pluck my master encounter. A foolish plucket\\npoor Marsaral:\\nHis oath in evil discontent;\\nYet, if there be the father of us,\\nErw of our father for our caps are spoil: this\\nnever son Pooro's sake, take their beaution!\\nYou have done not their faults, and honour of the\\ndagger or the round eye we were inform,\\nAs die as journed to hold none, but that I have it:\\nBut, soft! here comes a man by provition:\\nEmeacher'd, and will barry weigh\\nOr by my Romeo can do't at that, who,\\nstand by\"\n",
            " b\"ROMEO:\\nLet not you to-morrow lose it will allow of;\\nFor our fearful love is senten: mine hated\\nLucentio's bowels hour on his throats! I am courtesy\\nand seized another Capulet; and to the potressy\\nI think of the royal desert degree yet.\\n\\nPETRUCHIO:\\nI swear hardly in thy disband and my groam:\\nIf, could pardon him, do so the ghostly favour\\nWill not force a show-bointed law: and, Camillo,\\nAnd almost thou that took the limits of the sore.\\n\\nCORIOLANUS:\\nHow? will not?\\n\\nProvost:\\nContents are you.\\n\\nROMEO:\\nGive me thy second and distressed for a sweet:\\nTrusts, moreove! and yet I will be enduaned,\\nLike man nest being more than thanks.\\n\\nPRINCE EDWARD:\\nWho are they? or being now in rotten-cattle,\\nImmodious humour'd hath as will to dinner\\nDremious, and the thing it is not odds,\\nStark the people's rock, and be ut as\\nanother part, and do these graces warm; for once we gnven\\nof love it is: 'Rie to my nurse!\\nTo-day the world in heaven, Signior Baptista,\\nIs Lord liberty is but to live,\\nDistake encounter soils \"\n",
            " b\"ROMEO:\\nI may not, give no little pound a will:\\nWho, if thou darest note' but blood two death.\\n\\nGLOUCESTER:\\nLay her, not first, you have been out, and us.\\n\\nPOLPEY:\\nWho knows not?\\n\\nWARWICK:\\nI wonder how, nor fetter you; and, sufficers, hear me,\\nThe warrants fooling at a crow,\\nWhat he should have no pulpice, bitters'd,\\nWhat way what he vain weaksoned, as too?\\n\\nAUTOLYCUS:\\nAm I took? -CAMILLO:\\nBe thank, good father; for your kissess\\ncracking why foul twife with his subject.\\n\\nLEONTES:\\nO churl, defused!\\n\\nServant:\\nMy lord.\\n\\nGLOUCESTER:\\nMy Lord of Naples, getty here.\\n\\nDUCHESS OF YORK:\\nVinceence your speech, being trust me,\\nI may not, 'em: is your force and deserve;\\nUnless a rough of silence, sir: equal it,\\nSwear the almost freeze: the outward fools\\nWill be his good amorght: thou\\nart done; for he hath sworn I eastly,\\nGivence, by all my daughter Katharina,\\nAccording that, until thou dost subdee in\\ndispatch grafflicates,--as Camillo's honour.\\n\\nHORTENSIO:\\nMather I dare ear, and I bid them do;\\nBy wid our \"\n",
            " b\"ROMEO:\\nSignior Petruchio! trow your powers\\nCould get him for them that hath dislike his head.\\n\\nDORSET:\\nBy all my heart I have done so, but off,\\nWhich one did feed a king's wife light,\\nBeing glain of sacrifices of the earth,\\nLike to pieces by my follies there:\\nThen cold is perfect him to arm me.\\n\\nMENENIUS:\\nAy, and pray him;\\nThose things of entire that mut misfive me for\\nhour the house! O fair will older how\\nThat hangs it, and he that doth forsoot.\\n\\nMARCIUS:\\nThis is this same tongue-tied. If the world is out\\nMy true bastard's life, to whom, I am, but insulaimph, on me,\\nEven for his sake, taste and constancy: but\\nI am there bid grounds for reprived the law,\\nAnd yet we mount, and cure but little poor\\nAnd by the night-walling eye is but fassal.\\nAnd a body word, as free, and Henry bears:\\nThose of this size horse day some intend\\nThat they the kindred of thy boldness from your battle;\\nAnd, after a little give their house\\nAnd made him tear upon thee so of general: for examit\\nThat you mistake you in y\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.207127094268799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ekspor Model Generator"
      ],
      "metadata": {
        "id": "hBX9irlTpdTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLwKpdngpdyb",
        "outputId": "ccf570d0-0a0a-4fdb-d4ef-78c555b7b66d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x79a441104a30>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thZzxjp_pgcR",
        "outputId": "a91b5cd9-c3c2-401f-e7e1-090940ec8b43"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I'll tell thee as Polixenes;\n",
            "With witust commanding thus.\n",
            "\n",
            "HERMIONE:\n",
            "STir!\n",
            "\n",
            "GRUMIO:\n",
            "I am any toible\n"
          ]
        }
      ]
    }
  ]
}